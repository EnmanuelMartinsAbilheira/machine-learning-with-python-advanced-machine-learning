{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=6>Módulo 5. Fase de modelado</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=5>1. Algoritmos de Machine Learning</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Manuel Castillo-Cara</font><br>\n",
    "<font color=\"#004D7F\" size=3>Machine Learning con Python</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "<a id=\"indice\"></a>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "    * [1.1. Librerías](#section11)\n",
    "    * [1.2. CSV](#section12)\n",
    "* [2. Algoritmos de Clasificación](#section2)\n",
    "    * [2.1. Algoritmos de taxonomía lineal](#section21)\n",
    "    * [2.2. Algoritmos de taxonomía no lineal](#section22)\n",
    "* [3. Algoritmos de Regresión](#section3)\n",
    "    * [3.1. Algoritmos de taxonomía lineal](#section31)\n",
    "    * [3.2. Algoritmos de taxonomía no lineal](#section32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eel20\\AppData\\Local\\Temp\\ipykernel_21632\\1126351303.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container{ width:98% }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite ajustar la anchura de la parte útil de la libreta (reduce los márgenes)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container{ width:98% }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, vamos a analizar algunos de estos algoritmos, en especial, los de taxonomía lineal y no lineal. En cuanto a la taxonomía de conjunto o ensamblados, como los tipo boosting y bagging, los veremos posteriormente cuando ya tengamos una base sólida de estos primeros. Cada algoritmo será presentado desde dos perspectivas:\n",
    "* El paquete y la función utilizados para entrenar y hacer predicciones.\n",
    "* Las configuraciones en el paquete scikit-learn para cada algoritmo, estos es, la configuración de sus hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section11\"></a>\n",
    "## <font color=\"#004D7F\"> 1.1. Librerías</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a declarar algunas librerías generales que ya hemos estado trabajando y que usaremos a lo largo de la sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section12\"></a>\n",
    "## <font color=\"#004D7F\"> 1.2. CSV</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección, se muestran varias métricas de evaluación de algoritmos diferentes para problemas de Machine Learning de clasificación y regresión. En cada código, el conjunto de datos se descarga directamente del repositorio de UCI Machine Learning.\n",
    "* **Clasificación**: Se utilizará el conjunto de datos de Pima Indians Diabetes con una validación cruzada 10-folds para demostrar cómo verificar cada algoritmo de ML y se utilizan medidas de precisión promedio para indicar el rendimiento del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasification problem\n",
    "import pandas as pd\n",
    "filename = 'data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "df_cla = pd.read_csv(filename, names=names)\n",
    "array = df_cla.values\n",
    "X_cla = array[:,0:8]\n",
    "Y_cla = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Regresión**: Para ello utilizaremos el dataset Boston House Price. Este es un problema de regresión donde todos los atributos son numéricos. Se utiliza un arnés de prueba con validación cruzada 10 veces para demostrar cómo verificar cada algoritmo de aprendizaje automático y las medidas de error cuadrático medio se utilizan para indicar el rendimiento del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = 'data/housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df_reg = pd.read_csv(filename, delim_whitespace=True, names=names) \n",
    "array = df_reg.values\n",
    "X_reg = array[:,0:13]\n",
    "Y_reg = array[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\"> 2. Algoritmos de clasificación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comprobación puntual es una forma de descubrir qué algoritmos funcionan bien en su problema de ML. No puede saber qué algoritmos se adaptan mejor a su problema de antemano. Debe probar una serie de métodos y centrar la atención en los que resulten más prometedores. En este capítulo descubrirá seis algoritmos de ML que puede usar al verificar su problema de clasificación en Python con scikit-learn. Para ellos veremos dos taxonomías principales:\n",
    "lineal y no lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "## <font color=\"#004D7F\"> 2.1. Algoritmos de taxonomía lineal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a echar un vistazo a seis algoritmos de clasificación que puede verificar en su conjunto de datos. Comenzando con dos algoritmos lineales:\n",
    "* Logistic Regression (LoR).\n",
    "* Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section211\"></a>\n",
    "### <font color=\"#004D7F\"> 2.1.1. Logistic Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoR supone una distribución gaussiana para las variables de entrada numéricas y puede modelar problemas de clasificación binaria. Puede construir un modelo de regresión logística utilizando la clase `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.22% (4.97%)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "results = cross_val_score(model, X_cla, Y_cla, cv=kfold)\n",
    "print(f\"Accuracy: {results.mean()*100.0:,.2f}% ({results.std()*100.0:,.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section212\"></a>\n",
    "## <font color=\"#004D7F\"> 2.1.2. Linear Discriminant Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) es una técnica estadística para la clasificación binaria y multiclase. También supone una distribución gaussiana para las variables de entrada numéricas. Puede construir un modelo LDA utilizando la clase `LinearDiscriminantAnalysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.70% (4.80%)\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X_cla, Y_cla, cv=kfold)\n",
    "print(f\"Accuracy: {results.mean()*100.0:,.2f}% ({results.std()*100.0:,.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`LinearDiscriminantAnalysis`](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section22\"></a>\n",
    "## <font color=\"#004D7F\"> 2.2. Algoritmos de taxonomía no lineal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vamos a estudiar cuatro algoritmos de Machine Learning no lineales:\n",
    "* _k_-Nearest Neighbors ($k$-NN).\n",
    "* Naive Bayes (NB).\n",
    "* Classification and Regression Trees (CART). \n",
    "* Support Vector Machines (SVM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section221\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.1. $k$-Nearest Neighbours</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo $k$-Nearest Neighbours _($k$-NN)_ utiliza una métrica de distancia para encontrar las k instancias más similares en los datos de entrenamiento para una nueva instancia y toma el resultado medio de los vecinos como la predicción. Puede construir un modelo $k$-NN utilizando la clase `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -2,874.59% (1,200.18%)\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section222\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.2. Naive Bayes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes (NB) calcula la probabilidad de cada clase y la probabilidad condicional de cada clase dado cada valor de entrada. Estas probabilidades se estiman para datos nuevos y se multiplican juntos, suponiendo que todas sean independientes (una suposición simple o ingenua). Cuando se trabaja con datos de valor real, se supone que una distribución gaussiana estima fácilmente las probabilidades para las variables de entrada utilizando la función de densidad de probabilidad gaussiana. Puedes construir un modelo Naive Bayes usando la clase `GaussianNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.52% (4.28%)\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section223\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.3. Classification and Regression Trees</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de clasificación y regresión (CART) construyen un árbol binario a partir de los datos de entrenamiento. Los puntos divididos se eligen con avidez evaluando cada atributo y cada valor de cada atributo en los datos de entrenamiento para minimizar una función de costo (como el índice de Gini). Puedes construir un modelo CART usando la clase `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.83% (5.90%)\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section224\"></a>\n",
    "### <font color=\"#004D7F\"> 2.2.4. Support Vector Machine</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) buscan una línea que separe mejor dos clases. Las instancias de datos más cercanas a la línea que mejor separan las clases se denominan vectores de soporte e influyen en el lugar donde se ubica la línea. SVM se ha extendido para admitir múltiples clases. De particular importancia es el uso de diferentes funciones del núcleo a través del parámetro del núcleo. Por defecto, se utiliza una potente función de base radial. Puedes construir un modelo SVM usando la clase `SVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.04% (5.29%)\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\"> 3. Algoritmos de Regresión</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se estudiarán seis algoritmos de Machine Learning que puede usar al verificar su problema de regresión en Python con scikit-learn. Para ello, vamos a echar un vistazo a siete algoritmos de regresión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section31\"></a>\n",
    "## <font color=\"#004D7F\"> 3.1. Algoritmos de taxonomía lineal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos con cuatro algoritmos lineales:\n",
    "* Linear Regression (LiR).\n",
    "* Ridge Regression (RiR).\n",
    "* LASSO Linear Regression (LASSO). \n",
    "* Elastic Net Regression (ENR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section311\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.1. Linear Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LiR supone que las variables de entrada tienen una distribución gaussiana. También se supone que las variables de entrada son relevantes para la variable de salida y que no están altamente correlacionadas entre sí (un problema llamado colinealidad). Puede construir un modelo de LiR utilizando la clase `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: -23.74650181131339\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X_reg, Y_reg, cv=kfold, scoring= scoring)\n",
    "print(f\" MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section312\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.2. Ridge Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RiR es una extensión de LiR donde la función de pérdida se modifica para minimizar la complejidad del modelo medido como el valor de la suma cuadrática de los valores del coeficiente (también llamada norma __L2__). Puede construir un modelo RiR utilizando la clase `Ridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: -23.889890185053464\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X_reg, Y_reg, cv=kfold, scoring= scoring)\n",
    "print(f\" MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section313\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.3. LASSO Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO es una modificación de LiR, como RiR, donde la función de pérdida se modifica para minimizar la complejidad del modelo medido como el valor absoluto de los valores del coeficiente (también llamada la norma __L1__). Puede construir un modelo LASSO utilizando la clase `Lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: -28.74589007585154\n"
     ]
    }
   ],
   "source": [
    "# LASSO Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = Lasso()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X_reg, Y_reg, cv=kfold, scoring= scoring)\n",
    "print(f\" MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section314\"></a>\n",
    "### <font color=\"#004D7F\"> 3.1.4. ElasticNet Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENR es una forma de regresión de regularización que combina las propiedades de RIR y LASSO. Busca minimizar la complejidad del modelo de regresión (magnitud y número de coeficientes de regresión) penalizando el modelo utilizando tanto la norma L2 (valores de coeficiente de suma cuadrática) como la norma L1 (valores de coeficiente absoluto de suma). Puede construir un modelo ENR utilizando la clase `ElasticNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -31.164573714249762\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "model = ElasticNet()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X_reg, Y_reg, cv=kfold, scoring= scoring)\n",
    "print(f\" MSE: {results.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a>\n",
    "## <font color=\"#004D7F\"> 3.2. Algoritmos de taxonomía no lineal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, estudiaremos tres algoritmos no lineales:\n",
    "* $k$-Nearest Neighbors ($k$-NN).\n",
    "* Classification and Regression Trees (CART). \n",
    "* Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section321\"></a>\n",
    "### <font color=\"#004D7F\"> 3.2.1. $k$-Nearest Neighbours</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-NN localiza las $k$ instancias más similares en el conjunto de datos de entrenamiento para una nueva instancia de datos. De los $k$ vecinos, se toma una variable de salida media o mediana como la predicción. Cabe destacar la métrica de distancia utilizada (el argumento de la métrica). La distancia de _Minkowski_ se usa por defecto, que es una generalización tanto de la distancia euclidiana (utilizada cuando todas las entradas tienen la misma escala) como de la distancia de _Manhattan_ (para cuando las escalas de las variables de entrada difieren). Puede construir un modelo de $k$-NN utilizando la clase `KNeighborsRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -107.28683898039215\n"
     ]
    }
   ],
   "source": [
    "# k-NN Regressionore\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`KNeighborsRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section322\"></a>\n",
    "### <font color=\"#004D7F\"> 3.2.2. Classification and Regression Trees</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART usa los datos de entrenamiento para seleccionar los mejores puntos para dividir los datos con el fin de minimizar una métrica de costos. La métrica de costo predeterminada para los árboles de decisión de regresión es el error cuadrático medio, especificado en el parámetro `criterion`. Puede construir un modelo CART utilizando la clase `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -38.80579843137255\n"
     ]
    }
   ],
   "source": [
    "# CART Regression\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section323\"></a>\n",
    "### <font color=\"#004D7F\"> 3.2.3. Support Vector Machines</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM se desarrollaró para la clasificación binaria. La técnica se ha extendido para los problemas de predicción de valores reales llamados Support Vector Regression (SVR). Al igual que el ejemplo de clasificación, SVR se basa en la biblioteca `LIBSVM`. Puede construir un modelo SVR utilizando la clase `SVR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -91.04782433324428\n"
     ]
    }
   ],
   "source": [
    "# CART Regression\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`SVR`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

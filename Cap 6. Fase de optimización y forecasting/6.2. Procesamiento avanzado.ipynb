{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=6>Módulo 6. Fase de optimización y forecasting</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=5>2. Procesamiento de datos avanzado</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Manuel Castillo-Cara</font><br>\n",
    "<font color=\"#004D7F\" size=3>Machine Learning con Python</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "<a id=\"indice\"></a>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Valores Perdidos](#section2)\n",
    "* [3. Escalar el atributo clase](#section3)\n",
    "* [4. One-Hot Encoding](#section4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eel20\\AppData\\Local\\Temp\\ipykernel_3364\\1126351303.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container{ width:98% }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite ajustar la anchura de la parte útil de la libreta (reduce los márgenes)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container{ width:98% }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los transformadores son aquellos algoritmos y funciones que toman como datos de entrada las variables de nuestro problema y devuelven otras variables.\n",
    "\n",
    "Estas transformaciones suelen __aplicarse antes de un algoritmo de clasificación/regresión__, y todas aquellas operaciones que se ejecutan antes de un algoritmo de predicción reciben el nombre de __preprocesamiento__.\n",
    "\n",
    "En esta sección, aunque corresponda a la fase de procesamiento de datos, vamos a trabajar aspectos avanzados en el procesamiento de datos. Estos casos a estudiar son casos que nos vamos a encontrar muchísimo cuando trabajamos un proyecto de machine learning, además vamos a ver los resultados que nos dan los algoritmos para ver su mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a> \n",
    "# <font color=\"#004D7F\">2. Valores perdidos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las transformaciones más básicas consiste en la sustitución de valores perdidos o `NaN's` de la base de datos, ya que muchos algoritmos no son capaces de manejarlos. Una de las formas de realizar esta sustitución de valores perdidos consiste en utilizar la __media (para valores continuos) o la moda (caso discreto)__ con los casos conocidos y aplicar dicha métrica a los `NaN's` de la variable.\n",
    "\n",
    "La función `sklearn.impute.SimpleImputer` se encarga de calcular y modificar los datos de entrada. Vamos a ver cómo se utiliza en el caso de Wisconsin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a> \n",
    "## <font color=\"#004D7F\">2.1. Caso genérico</font>\n",
    "\n",
    "Lo primero que vamos a realizar es ver que atributos tienen valores perdidos, para ello vamos a utilizar las funciones de NumPy que nos permite analizarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patientId            0\n",
      "clumpThickness       0\n",
      "cellSize             0\n",
      "CellShape            0\n",
      "marginalAdhesion     0\n",
      "epithelialSize       0\n",
      "bareNuclei          32\n",
      "blandChromatin       0\n",
      "normalNucleoli       0\n",
      "mitoses              0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eel20\\AppData\\Local\\Temp\\ipykernel_3364\\1322922485.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  wisconsin_data = wisconsin.drop('label', 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "wisconsin = pd.read_csv('data/wisconsin.csv', dtype={ \"label\": 'category'})\n",
    "wisconsin\n",
    "# Lo primero es separar los atributos de la clase. 1 es por la columna\n",
    "wisconsin_data = wisconsin.drop('label', 1)\n",
    "wisconsin_target = wisconsin['label']\n",
    "\n",
    "# Después comprobamos los NaN's de nuestros datos, esto podemos hacerlo con Numpy.\n",
    "print(np.sum(np.isnan(wisconsin_data)))\n",
    "\n",
    "# Vemos que BareNuclei es el único que tiene 32 valores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo la variable `bareNuclei` tiene valores perdidos, por lo que vamos a aplicar el `SimpleImputer` y ver que resultados se obtienen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Importante: Debido a los `NaNs` el algoritmo no podrá ejecutarse, por lo que es necesario utilizar alguna de las bases de datos `wisconsin` a las que se les ha aplicado el `Imputer` en la sección anterior o volver a aplicarlo ahora.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-898d4508a94e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# entrenamos con los datos de entrada y la salida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwisconsin_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwisconsin_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Este algoritmo no trabaja con NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Jupyter/enviroment/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1525\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Jupyter/enviroment/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    737\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    740\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/Documentos/Jupyter/enviroment/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 562\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Jupyter/enviroment/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Importamos la clase LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# creamos un objeto con los parámetros por defecto\n",
    "model = LogisticRegression()\n",
    "# entrenamos con los datos de entrada y la salida\n",
    "model.fit(wisconsin_data, wisconsin_target)\n",
    "# Este algoritmo no trabaja con NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, los valores perdidos han sido sustituidos por la media de los valores conocidos en `bareNuclei`. Si vamos a la documentación del [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer), podremos observar los diferentes parámetros de configuración del algoritmo. Si queremos modificar el comportamiento del algoritmo deberemos definir estos parámetros en el constructor cuando creemos el objeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patientId           0\n",
      "clumpThickness      0\n",
      "cellSize            0\n",
      "CellShape           0\n",
      "marginalAdhesion    0\n",
      "epithelialSize      0\n",
      "bareNuclei          0\n",
      "blandChromatin      0\n",
      "normalNucleoli      0\n",
      "mitoses             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creamos un objeto de la clase Imputer con los parámetros por defecto\n",
    "imp = SimpleImputer(strategy='median')\n",
    "\n",
    "# Llamamos a fit, para que aprenda la media con la que tiene que rellenar los NaN's y le pasamos los datos\n",
    "imp.fit(wisconsin_data)\n",
    "\n",
    "# Finalmente, transformamos los datos con las medias aprendidas.\n",
    "wisconsin_trans = imp.transform(wisconsin_data)\n",
    "\n",
    "# Aunque le hayamos pasado un pandas a SciKit, este nos devuelve un numpy array, por lo que volvemos a \n",
    "# trasnformarlo a pandas\n",
    "wisconsin_trans = pd.DataFrame(wisconsin_trans, columns = wisconsin_data.columns)\n",
    "wisconsin_trans\n",
    "\n",
    "# Si volvemos a comprobar los NaN's en la nueva base de datos, vemos que ya no hay\n",
    "print(np.sum(np.isnan(wisconsin_trans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si que podemos lanzar el algoritmo ya que no tenemos ningún valor NaN en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9699570815450643\n"
     ]
    }
   ],
   "source": [
    "# Importamos la clase LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# creamos un objeto con los parámetros por defecto\n",
    "model = LogisticRegression(solver = 'lbfgs', max_iter=1000)\n",
    "\n",
    "# entrenamos con los datos de entrada y la salida\n",
    "model.fit(wisconsin_trans, wisconsin_target)\n",
    "\n",
    "# obtenemos una predicción para los datos de wisconsin\n",
    "model_pred = model.predict(wisconsin_trans)\n",
    "score = model.score(wisconsin_trans, wisconsin_target)\n",
    "\n",
    "# Otenemos el Accuracy\n",
    "print(f\"Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section22\"></a>\n",
    "## <font color=\"#004D7F\"> 2.2. Pipeline con `SimpleImputer` </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el este ejemplo vamos a definir un Pipeline con algunas de las clases que hemos utilizado. Vamos a utilizar `wisconsin_data`, que si recordamos, tenía ciertos valores perdidos, por lo que no podía ser pasado directamente al clasificador. Por eso, vamos a crear un Pipeline que primero utilice un `SimpleImputer` y luego llame a `LogisticRegression`.\n",
    "\n",
    "Igual que en todos los casos anteriores, utilizaremos el Pipeline con las funciones `fit`, `predict` y `score`, de la misma forma que si se tratase de un clasificador. Para crear un Pipeline se utiliza una lista de tuplas (clave, valor) donde la clave es un string representativo y el valor es el estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto: 0.9699570815450643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([('Imp', SimpleImputer()),\n",
    "                ('LoR', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "pipe.fit(wisconsin_data, wisconsin_target)\n",
    "print(f\"Tasa de acierto: {pipe.score(wisconsin_data, wisconsin_target)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\"> 3. Escalar atributo clase(Target)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En problemas de modelado predictivo de regresión donde se debe predecir un valor numérico, también puede ser crítico escalar y realizar otras transformaciones de datos en la variable objetivo. Esto se puede lograr en Python usando la clase `TransformedTargetRegressor`. Para problemas de regresión, a menudo es deseable escalar o transformar tanto las variables de entrada como las de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`TransformedTargetRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html?highlight=transformedtargetregressor#sklearn.compose.TransformedTargetRegressor).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of normalizing input and output variables for regression.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "filename = 'data/housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = pd.read_csv(filename, delim_whitespace=True, names=names) \n",
    "array = dataframe.values\n",
    "#X las Caracteristicas\n",
    "X = array[:,0:13]\n",
    "#Y son los Target (clases)\n",
    "Y = array[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.2095842117833655\n"
     ]
    }
   ],
   "source": [
    "# prepare the model with input scaling\n",
    "## en este conjunto de datos aplicara (una transformacion MinMaxScaler a las caracteristicas) y luego\n",
    "## applicar un algoritmo de machine learning (HuberRegressor) \n",
    "pipeline = Pipeline(steps = [('Nomalize', MinMaxScaler()), ('HuR', HuberRegressor())])\n",
    "\n",
    "# prepare the model with target scaling\n",
    "## para utilizasr el pipeline donde aplicara las caracteristicas y ademas\n",
    "## aplicara una transformacion en los dartos con MinMaxScaler\n",
    "model = TransformedTargetRegressor(regressor=pipeline, transformer= MinMaxScaler())\n",
    "\n",
    "# evaluate model\n",
    "cv =KFold(n_splits=10, shuffle=True, random_state=7)\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv)\n",
    "\n",
    "# convert scores to positive\n",
    "## hace que el resultado final nunca sea negativo\n",
    "scores = np.absolute(scores)\n",
    "\n",
    "# summarize the result\n",
    "s_mean = np.mean(scores)\n",
    "print(f\"MAE: {s_mean}\")\n",
    "\n",
    "## el resultado en este caso MAE: 3.2095842117833655 es el error medio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si, por ejemplo ahora le hacemos una transformación de Yeo-Johnson vemos como mejoran nuestros resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 2.926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "# prepare the model with input scaling\n",
    "???\n",
    "# prepare the model with target scaling\n",
    "???\n",
    "# evaluate model\n",
    "???\n",
    "# convert scores to positive\n",
    "???\n",
    "# summarize the result\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\"> 4. One-Hot Encoding</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar transformaciones de datos como escalar o codificar variables categóricas es sencillo cuando todas las variables de entrada son del mismo tipo. Puede ser un desafío cuando tiene un conjunto de datos con tipos mixtos y desea aplicar transformaciones de datos selectivamente a algunas, pero no a todas, las características de entrada.\n",
    "\n",
    "Afortunadamente, la biblioteca de aprendizaje automático Python scikit-learn proporciona el `ColumnTransformer` que le permite aplicar transformaciones de datos de forma selectiva a diferentes columnas de su conjunto de datos.\n",
    "\n",
    "Para este ejemplo vamos a utilizar el conjunto de datos [Abalone](https://archive.ics.uci.edu/ml/datasets/Abalone) el cual es un problema de clasificación pero tiene características numéricas y categóricas que tenemos que transformar para que todas se encuentren en un mismo tipo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "Documentación oficial de la clase [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.465 (0.047)\n"
     ]
    }
   ],
   "source": [
    "# example of using the ColumnTransformer for the Abalone dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "# load dataset\n",
    "filename = 'data/abalone.csv'\n",
    "dataframe = pd.read_csv(filename, header=None) \n",
    "array = dataframe.values\n",
    "# split into inputs and outputs\n",
    "???\n",
    "# determine categorical and numerical features\n",
    "???\n",
    "# define the data preparation for the columns\n",
    "???\n",
    "# define the model\n",
    "???\n",
    "# define the data preparation and modeling pipeline\n",
    "???\n",
    "# define the model cross-validation configuration\n",
    "???\n",
    "# evaluate the pipeline using cross validation and calculate MAE\n",
    "???\n",
    "# convert MAE scores to positive values\n",
    "???\n",
    "# summarize the model performance\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, logramos un MAE promedio de aproximadamente 1.4, que es mejor que el puntaje inicial de 2.3. Es por eso que es muy importante tener todos los atributos del mismo tipo, porque los algoritmos, sobre todo los basados en funciones o representaciones en funciones mejoran considerablemente.\n",
    "\n",
    "Los algoritmos tipo árboles tienen un comportamiento contrario, es decir, prefieren que los atributos sean tipo categóricos aunque su desempeño no mejora tanto como los basados en funciones a pasar a numérico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5>\n",
    "    <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a>\n",
    "</font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
